《深度学习进阶：自然语言处理》笔记

# 词的表示

根据词在文章中表征出的现象，提取出词的特征

## 同义词词典

通过人工的方式标注词性，将句子解析成树状的结构

WordNet、知网

缺点：难以顺应时代变化、成本高、无法表示单词的微妙差异

## 计数统计

由词汇在文章中的分布特征，生成词向量

### 上下文相关性——共现矩阵（co-occurrence matrix）

窗口大小：上下文范围（当窗口大小为1时，左右各1个单词为上下文）

> You say goodbye and I say hello.

| co-occurrence matrix（窗口大小为1） | you  | say  | goodbye | and  | i    | hello | .    |
| ----------------------------------- | ---- | ---- | ------- | ---- | ---- | ----- | ---- |
| you                                 | 0    | 1    | 0       | 0    | 0    | 0     | 0    |
| say                                 | 1    | 0    | 1       | 0    | 1    | 1     | 0    |
| goodbye                             | 0    | 1    | 0       | 1    | 0    | 0     | 0    |
| and                                 | 0    | 0    | 1       | 0    | 1    | 0     | 0    |
| i                                   | 0    | 1    | 0       | 1    | 0    | 0     | 0    |
| hello                               | 0    | 1    | 0       | 0    | 0    | 0     | 1    |
| .                                   | 0    | 0    | 0       | 0    | 0    | 1     | 0    |

`C(you, say)=1`，说明you和say共现过1次。

`C(say)=4`说明say共有4个。

### 点互信息（Pointwise Mutual Information - PMI）

$$
PMI(x,y)=\log_2\frac{P(x,y)}{P(x)P(y)}=\log_2\frac{\frac{C(x,y)}{N}}{\frac{C(x)}{N}\frac{C(y)}{N}}=\log_2\frac{C(x,y) \times N}{C(x)C(y)}
$$

为了PMI非负，小于0的统一为0

$$
PPMI(x,y)=\max(0,PMI(x,y))
$$

PPMI为(N,N)的对称方阵，由于N为单词种类数。

| PPMI（窗口大小为1） | you  | say  | goodbye | and  | i    | hello | .    |
| ------------------- | ---- | ---- | ------- | ---- | ---- | ----- | ---- |
| you                 | 0    | 1.81 | 0       | 0    | 0    | 0     | 0    |
| say                 | 1.81 | 0    | 0.81    | 0    | 0.81 | 0.81  | 0    |
| goodbye             | 0    | 0.81 | 0       | 1.81 | 0    | 0     | 0    |
| and                 | 0    | 0    | 1.81    | 0    | 1.81 | 0     | 0    |
| i                   | 0    | 0.81 | 0       | 1.81 | 0    | 0     | 0    |
| hello               | 0    | 0.81 | 0       | 0    | 0    | 0     | 2.81 |
| .                   | 0    | 0    | 0       | 0    | 0    | 2.81  | 0    |

### SVD降维

随着语料库的增大，矩阵会变得十分庞大。

可以考虑用SVD降维：

$$
\underset{(m \times n)}{\mathrm{M}} =
\underset{(m \times m)}{\mathrm{U}} \times
\underset{(m \times n)}{\scriptstyle{\sum}} \times
\underset{(n \times n)} {\mathrm{V^T}}
$$

![SVD_1](<img/SVD_1.jpg>)

中间的矩阵为特征值矩阵，将特征值按从大到小排序，并将左右奇异矩阵依次对应排序。

特征值矩阵下方空白的几行可以删去，剩下宽度为n的方阵，同时删掉左奇异矩阵的右侧几列。

再进行降维，删减掉特征值较小的部分，保留下r个最大的特征值的方阵，同时删掉右奇异矩阵的下方n-r行。

$$
\underset{(m \times n)}{\mathrm{M}} =
\underset{(m \times r)}{\mathrm{U}} \times
\underset{(r \times r)}{\scriptstyle{\sum}} \times
\underset{(r \times n)} {\mathrm{V^T}}
$$

![SVD_2](<img/SVD_2.jpg>)

（猜测）左右奇异矩阵分别表示行空间与列空间的特征。[参考阅读](#参考阅读 文本处理中的分类问题)

![SVD_7](<img/SVD_7.png>)

可以利用左右奇异矩阵压缩原始矩阵，用左奇异矩阵压缩原始矩阵的行，用左奇异矩阵压缩原始矩阵的列：
$$
\underset{(r \times n)}{\mathrm{M'}} =
\underset{(r \times m)}{\mathrm{U^T}} \times
\underset{(m \times n)}{\mathrm{M}}
$$

$$
\underset{(m \times r)}{\mathrm{M'}} =
\underset{(m \times n)}{\mathrm{M}} \times
\underset{(n \times r)} {\mathrm{V}}
$$

这里用SVD求出PPMI的左\右奇异矩阵（U和V），即降维后的(N,k) 行空间\列空间 的特征矩阵，k为选择最大的特征数。选取特征矩阵中的前2维（这里因为M是对称矩阵，所以U和V所代表的含义相同，选其中一个就行了），作2D图以示特征的相关性（相邻的点表示相关性高）：

![SVD_8](<img/SVD_8.jpeg>)

#### 参考阅读 文本处理中的分类问题

最后就要分析在《数学之美》之美的矩阵计算与文本处理中的分类问题一章节中，吴军老师讲到：

> “三个矩阵有非常清楚的物理含义。第一个矩阵X中的每一行表示意思相关的一类词，其中的每个非零元素表示这类词中每个词的重要性（或者说相关性），数值越大越相关。最后一个矩阵Y中的每一列表示同一主题一类文章，其中每个元素表示这类文章中每篇文章的相关性。中间的矩阵则表示词的类和文章的类之间的相关性。因此，我们只要对关联矩阵A进行一次奇异值分解，可以同时完成了近义词分类和文章的分类。（同时得到每类文章和每类词的相关性）。”

实际上在读的时候我是没有理解的，这是潜在语义索引（Latent Semantic Indexing）的精髓内容，下面借用一个博客中的例子说明下：

![此处输入图片的描述](<img/201101192226386634.png>)

一行表示一个词在哪些title中出现了（一行就是之前说的一维feature)，一列表示一个title中有哪些词。比如说T1这个title中就有guide、investing、market、stock四个词，各出现了一次，将这个矩阵进行SVD，得到下面的矩阵：

![此处输入图片的描述](<img/201101192226397148.png>)

左奇异向量表示词的一些特性，右奇异向量表示文档的一些特性，中间的奇异值矩阵表示左奇异向量的一行与右奇异向量的一列的重要程度，数字越大越重要。
继续看这个矩阵还可以发现一些有意思的东西，首先，左奇异向量的第一列表示每一个词的出现频繁程度，虽然不是线性的，但是可以认为是一个大概的描述，比如book是0.15对应文档中出现的2次，investing是0.74对应了文档中出现了9次，rich是0.36对应文档中出现了3次；其次，右奇异向量中一的第一行表示每一篇文档中的出现词的个数的近似，比如说，T6是0.49，出现了5个词，T2是0.22，出现了2个词。

将左奇异向量和右奇异向量都取后2维（之前是3维的向量，直接取后两维度)，投影到一个平面上，可以得到：

![此处输入图片的描述](<img/201101192226404739.png>)

在图上，每一个红色的点，都表示一个词，每一个蓝色的点，都表示一篇文档，这样我们可以对这些词和文档进行聚类，比如说stock 和 market可以放在一类，因为他们老是出现在一起，real和estate可以放在一类，dads，guide这种词就看起来有点孤立了，我们就不对他们进行合并了。按这样聚类出现的效果，可以提取文档集合中的近义词，这样当用户检索文档的时候，是用语义级别（近义词集合)去检索了，而不是之前的词的级别。一是减少我们的检索、存储量，因为这样压缩的文档集合和PCA是异曲同工的；二是可以提高我们的用户体验，用户输入一个词，我们可以在这个词的近义词的集合中去找，这是传统的索引无法做到的。

## 基于Word Bag的推理——Wod2Vec

上面基于计数统计产生的词表示没有考虑上下文，这一章用神经网络学习上下文的信息，并产生词向量。

用one-hot的vector构成一个$N*1$的词袋，把上下文的单词的one-hot vector作为输入喂到单层的神经网络中，试图让神经网络预测输出中间词，这样经过大量训练后input layer中的weight就等价于每个词的向量。

![CBOW](<img/CBOW.jpg>)

- CBOW（多输入，1输出）
- skip-gram（1输入，多输出）

当N过大时，输入层和输出层的计算量会过大，考虑下面两个改进：

- 由于输入层是one-hot向量，W~in~可以看作只是提取其中一行的向量的一层，就省去了矩阵乘法（这被称为Word Embedding层）
- 因为中间层的所有参数对输出层都有影响，所以W~out~没办法像输入层那样简化。为了减少计算，只采样出W~out~中正确标签的词的那行，再随机出频率较大的5-10个词几行作为错误标签，算出loss更新网络。

## GloVe

结合了计数和推理

# 词序列的表示

## 朴素RNN

输入输出可变长，包含了时序的信息，解决了词袋模型巨量参数的问题

### 小trick

1. dropout
2. 权重共享

## 门控RNN

增加了记忆单元LSTM，解决了朴素RNN在时间方向上的梯度消失的问题

LSTM可以进行：

- 文本分类
- 文本生成，即一个字一个字蹦豆。

## Seq2Seq

如果想要进行序列到序列的生成，LSTM就无能了，比如：

- 翻译
- 写文章摘要
- 特定领域的聊天机器人
- 图像转文本描述

Seq2Seq，编码到解码，编码（输入变长个数的词汇） - 码（定长） - 解码器（与输入相同长度的个数）

### 小trick

1. Reverse（输入和训练标签的str同时翻转）
2. Peeky（解码器中所有的LSTM和Affine分到编码器输出的码，并和之前的输入相接，形成一个更长的输入）
3. 编码器双向RNN（输入的数据正着来一遍，反着来一半，输出值由两者拼接形成）

## Attention

Seq2Seq中编码器到解码器中间的对接码是定长且每次只有一个的，想要增强表征能力，可以将编码器中的所有LSTM的输出值h都喂给解码器（比如输入10个词汇，每个LSTM输出的h长度为4，然后把10x4的h矩阵喂给译码器），译码器中的各个LSTM的h'再与h矩阵计算相关度，作为权重计算输出值。

![Attention1](<img/Attention1.jpg>)

![Attention2](<img/Attention2.jpg>)

## 高级东西

### NTM
